{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3c4effd-3d0a-4670-a611-7751a9b0c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from wordcloud import WordCloud, STOPWORDS \n",
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import nltk\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d43503d5-4e98-4b22-b22d-df5f5740a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e10b0dc-8f81-4dfe-9b12-2a05bc07e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc10990-ac58-48de-b117-c9fb581c3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk==3.8.1 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e868080-f87f-4610-80b8-eac8372b48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data/train.csv'\n",
    "size_vocab = 5000\n",
    "folder_path = 'tweets_' + str(size_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "290c3122-3351-455a-ab59-36bc15db09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(file_name):\n",
    "    '''\n",
    "    input: csv file with keyword, location, and text columns\n",
    "    output: x_train_padded, y_train, x_valid_padded, y_valid, X_test, y_test, word_list\n",
    "        x_train_padded: training data (60% of total data) \n",
    "        y_train: labels of the training data\n",
    "        x_valid_padded: validation data (20% of total data)\n",
    "        y_valid: labels of the validation data\n",
    "        X_test: hold-out test data (20% of total data)\n",
    "        y_test: labels of the hold-out test data\n",
    "        word_list: collection of tokenized words without stop words from training and validation data\n",
    "    '''\n",
    "    \n",
    "    #There is an imbalance in true and false data. Disaster tweets have less than non-disater ones.\n",
    "    #By adding randomly selected disaster tweets (duplicates), the total data set will be balanced.\n",
    "    def pos_oversampling(df):\n",
    "        add_pos = 4342 - 3271\n",
    "        pos_df = df[df[\"target\"] == 1]\n",
    "        neg_df = df[df[\"target\"] == 0]\n",
    "        pos_df.reset_index(inplace=True)\n",
    "        pos_add_indices = np.random.choice(pos_df.index, add_pos, replace=False)\n",
    "        pos_add_df = pos_df.iloc[pos_add_indices]\n",
    "        pos_oversampled = pd.concat([pos_df, pos_add_df], ignore_index=True)\n",
    "        balanced_df = pd.concat([neg_df, pos_oversampled], ignore_index=True)\n",
    "        return balanced_df\n",
    "      \n",
    "    def tokenize_tweets(tweets):\n",
    "        tokenized_tweets = []\n",
    "        for tweet in tweets:\n",
    "            tweet = re.sub(r'[,!?;-]', '.', tweet) #  Punctuations are replaced by \".\"\n",
    "            #tweet_lowered = tweet.lower()\n",
    "            tokenized_tweet = nltk.word_tokenize(tweet) \n",
    "            #  Lower case and drop non-alphabetical tokens\n",
    "            tokenized_tweet = [ch.lower() for ch in tokenized_tweet if ch.isalpha() or ch == '.']  \n",
    "            tokenized_tweets.append(tokenized_tweet)\n",
    "        return tokenized_tweets\n",
    "\n",
    "    def stopwords_tweets(tokenized_tweets):\n",
    "        english_stopwords = stopwords.words('english')\n",
    "        tokens_wo_stopwords_tweets = []\n",
    "        for tokenized_tweet in tokenized_tweets:\n",
    "            tokens_wo_stopwords = [t for t in tokenized_tweet if t not in english_stopwords] \n",
    "            tokens_wo_stopwords_tweets.append(tokens_wo_stopwords) \n",
    "        return tokens_wo_stopwords_tweets\n",
    "\n",
    "    # flatten the embedded lists to create one long word list.\n",
    "    def flatten(xss):\n",
    "        return [x for xs in xss for x in xs]\n",
    "\n",
    "    # Here tweet in the argument is a tokenized tweet without stop words.\n",
    "    def padded_vector(tweet, vocab_dict, max_len):\n",
    "        int_tweet = []\n",
    "        if len(tweet) <= max_len:\n",
    "            for word in tweet:\n",
    "                if word in vocab_dict:\n",
    "                    int_tweet.append(vocab_dict[word])  \n",
    "        else:  # tweet is longer than maximum length \n",
    "            for word in tweet[:max_len]:  # truncate the tweet\n",
    "                if word in vocab_dict:\n",
    "                    int_tweet.append(vocab_dict[word])             \n",
    "        padded_vector = int_tweet + [0] * max(0, max_len - len(int_tweet))\n",
    "        return padded_vector\n",
    "    \n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "    # oversample the positive tweets\n",
    "    balanced_df = pos_oversampling(df)\n",
    "    \n",
    "    # replace empty cells with a strin \"NA\"\n",
    "    balanced_df.fillna('NA', inplace=True)\n",
    "\n",
    "    # Concatenate keyword, location, and text and name the new column as tweet.\n",
    "    balanced_df['tweet'] = balanced_df['keyword'] + ' ' + balanced_df['location'] + ' ' + balanced_df['text']\n",
    "\n",
    "    # splitting data; train 60%, valid 20%, and test 20%\n",
    "    X = balanced_df['tweet'].values\n",
    "    y = balanced_df['target'].values\n",
    "    X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=0.20, random_state=38)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_tr, y_tr, test_size=0.25, random_state=28)\n",
    "\n",
    "    #Create word corpus using X_train\n",
    "    tokenized_tweets_train = tokenize_tweets(X_train)\n",
    "    tokenized_tweets_train_wo_stopwords = stopwords_tweets(tokenized_tweets_train)\n",
    "    word_list = flatten(tokenized_tweets_train_wo_stopwords)\n",
    "    vocab_count = Counter(word_list)\n",
    "\n",
    "    #Let's use 1000 most popular words\n",
    "    vocab =  sorted(vocab_count,key=vocab_count.get,reverse=True)[:size_vocab]\n",
    "    vocab_dict = {w:i+1 for i,w in enumerate(vocab)}\n",
    "\n",
    "    #Tweet length statistics in train, valid, and test sets\n",
    "    tokenized_tweets_valid = tokenize_tweets(X_valid)\n",
    "    tokenized_tweets_valid_wo_stopwords = stopwords_tweets(tokenized_tweets_valid)\n",
    "    tokenized_tweets_test = tokenize_tweets(X_test)\n",
    "    tokenized_tweets_test_wo_stopwords = stopwords_tweets(tokenized_tweets_test)\n",
    "\n",
    "    # maximum length of tweets \n",
    "    max_len = max(len(w) for w in tokenized_tweets_train_wo_stopwords + tokenized_tweets_valid_wo_stopwords + tokenized_tweets_test_wo_stopwords)\n",
    "\n",
    "    # Using this max_len, let's build padded vectors\n",
    "    x_train_padded = [padded_vector(x, vocab_dict, max_len) for x in tokenized_tweets_train_wo_stopwords]\n",
    "    x_valid_padded = [padded_vector(x, vocab_dict, max_len) for x in tokenized_tweets_valid_wo_stopwords]\n",
    "\n",
    "    return x_train_padded, y_train, x_valid_padded, y_valid, X_test, y_test, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d998630-59d1-4791-a6cd-9e32b4a0c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded, y_train, x_valid_padded, y_valid, X_test, y_test, word_list = data_preprocess(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3c29e24-db06-4906-b815-719dff5a6433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The length of train padded: 5210\n",
      " The length of valid padded: 1737\n"
     ]
    }
   ],
   "source": [
    "print(f\" The length of train padded: {len(x_train_padded)}\")\n",
    "print(f\" The length of valid padded: {len(x_valid_padded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c3a79c6-a8ab-47d1-a72b-0562c2bcfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_csv(list_data, folder_path, file_name):\n",
    "    df_list = pd.DataFrame(list_data)\n",
    "    df_list.to_csv(f'{folder_path}/{file_name}', index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b151c5-2e03-40b1-828d-c26a5f6f0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pre-processed data as csv files for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67485a32-0ff7-4c9e-ab37-de9cc7ed2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_csv(x_train_padded, folder_path, 'x_train_padded.csv')\n",
    "list_to_csv(y_train, folder_path, 'y_train.csv')\n",
    "list_to_csv(x_valid_padded, folder_path, 'x_valid_padded.csv')\n",
    "list_to_csv(y_valid, folder_path, 'y_valid.csv')\n",
    "list_to_csv(X_test, folder_path, 'X_test.csv')\n",
    "list_to_csv(y_test, folder_path, 'y_test.csv')\n",
    "list_to_csv(word_list, folder_path, 'word_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd187d-e65d-4a3d-a296-97d1302d2d31",
   "metadata": {},
   "source": [
    "### Create final dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb6fd479-6faf-41c4-90a7-b44c0a7f3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e437937-6983-49fc-b84b-2e1f20f7c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_padded = np.array(x_train_padded)\n",
    "x_valid_padded = np.array(x_valid_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45275457-0ad2-433d-aa64-b209b4c2fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_padded), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_valid_padded), torch.from_numpy(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a107b845-6564-4be5-a433-a009df92cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataloader with shuffle on\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d011f444-5556-464e-a0ad-e3d30163adf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([50, 28])\n",
      "Labels batch shape: torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Display tweet and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45570a08-bf7f-4b74-9fc8-31e2a566c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: tensor([ 220,  691,  507,   15,    1, 1681,    1,  566,  220,  690,  909,    1,\n",
      "        1326,  647,  551, 1682,  460,    1,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "tweet_0 = train_features[0].squeeze()\n",
    "label_0 = train_labels[0]\n",
    "print(f\"Tweet: {tweet_0}\\nLabel: {label_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5cc48d-13c4-48d6-91b0-142fab41b415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c2f84-f112-4e6a-978c-2e3663c92c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skorch-310",
   "language": "python",
   "name": "skorch-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
